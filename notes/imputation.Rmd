---
title: "Imputation with Amelia"
author: "Eric R. Scott"
date: "2020-07-21"
output: 
  html_notebook: 
    highlight: kate
    theme: yeti
    toc: yes
    toc_float: yes
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
library(conflicted)
library(tsibble)
library(lubridate)
library(janitor)
library(Amelia)
library(parallel)

conflict_prefer("filter", "dplyr")
conflict_prefer("lag", "dplyr")
```

*Last compiled: `r Sys.Date()`*


# TODO:

- Figure out how easy it is to mutiply impute, then calculate SPI, then *combine results*
- Apply to entire dataset

# Purpose

Explore options for imputation of missing values.  In earlier versions of this document, I explored several options for imputation.  The `mice` package does multiple, multivariate imputation but it performed worse than simple methods like just using the mean.  The `mtsdi` package is for multivariate timeseries data, but it was too slow to be useable and poorly documented.  `Amelia` is well documented, and has functions to work with cross-sectional timeseries data.  It seems flexible, fast, and has built-in diagnostic features.

# Load & Wrangle Data

Because of the high degree of missingness in the BDFFP data, I'll combine a few sites into "clusters". The `Amelia` docs suggest using more variables rather than fewer, as it will increase the predictive power of the imputation algorithm. I'll therefore combine the BDFFP data with GPCC gridded data and Manaus station data (including variables besides precip). 

```{r data, echo=TRUE}
bdffp <- read_csv(here("data_cleaned", "daily_precip.csv"))
sa <- read_csv(here("data_cleaned", "sa_daily_1x1.csv"))
xa <- read_csv(here("data_cleaned", "xavier_daily_precip_0.25x0.25.csv"))
manaus <- read_csv(here("data_cleaned", "manaus_weather.csv"))
min(bdffp$date)
```

## Remove accumulations

I'll remove ALL accumulations for an initial test and impute those values.  `Amelia` has the ability to take informative priors about individual observations, so in the future maybe I can use accumulated values to inform preceeding `NA`s.

```{r}
bdffp2 <-
  bdffp %>% 
  filter(!flag %in% c("A", "U"))
```

## Make wide

I'm going to treat each site as a variable so `Amelia` can take advantage of information from all the sites. For that, I need a wide dataset.

```{r}
bdffp_wide <- 
  bdffp2 %>% 
  #complete all the dates
  as_tsibble(key = site, index = date) %>% 
  fill_gaps() %>% 
  select(date, site, precip) %>% 
  as_tibble() %>% 
  pivot_wider(names_from = site, values_from = precip) %>% 
  clean_names()
```

## Add additional data sources

```{r}
xa_wide <-
  xa %>% 
  mutate(xa_latlon = paste(lat, lon, sep = ", "), lat = NULL, lon = NULL) %>% 
  pivot_wider(names_from = xa_latlon, values_from = precip) %>%
  clean_names()
```


I'll join and remove a few variables with weird distributions

```{r}
full_wide <-
  left_join(bdffp_wide, rename(manaus, manaus = precip), by = "date") %>%
  left_join(select(sa, date, sa = precip), by = "date") %>% 
  left_join(xa_wide, by = "date") %>% 
  mutate(year = year(date), doy = yday(date)) %>%
  select(year, doy, everything()) %>%
  select(-temp_max, -temp_min, -sun_time)
```

Because BDFFP precip data is generally read at 7am, I'd expect it to be out of phase from Manaus by about a day.

## Site Clusters

Use these sites:

1. dimona
2. Porto alegre
3. colosso_clust = Colosso, florestal, cabo frio, gaviao
4. km_clust = km37, km41

```{r}
full_wide2 <- 
  full_wide %>% 
  rowwise() %>% 
  mutate(colosso_clust = mean(c(colosso, florestal, cabo_frio, gaviao), na.rm = TRUE),
         km_clust = mean(c(km37, km41), na.rm = TRUE)) %>% 
  mutate(across(colosso_clust:km_clust, ~ifelse(is.nan(.x), NA, .x))) %>% 
  select(-cabo_frio, -colosso, -florestal, -gaviao, -km37, -km41)
```


# Impute with Amelia II

## Test data

Make a data frame for testing that is a subset of a few years.  I'll also make on that only has precip data in it, for testing.

```{r}
slice_wide <- full_wide2 %>% filter(between(year, 1990, 2000)) %>% as.data.frame()
slice_precip <- slice_wide %>% select(year, doy, date:manaus, colosso_clust, km_clust)
```



## Simple

Only include sites at BDFFP and manaus station.  No lags or leads.

```{r}
imp_ts1 <-
  amelia(
    slice_precip,
    p2s = 0,
    m = 10,
    ts = "doy",
    cs = "year",
    intercs = TRUE,
    polytime = 3,
    logs = c("dimona", "porto_alegre", "km_clust", "colosso_clust", "manaus"),
    idvars = c("date"),
    empri = .01 * nrow(slice_precip), #ridge penalty because of high degree of missingness
    parallel = "multicore",
    ncpus = detectCores() - 1
  )
```


```{r}
imp_ts1
```

The chain lengths are fairly even, indicating that the ridge penalty is helping convergence.

## More vars

Now including all the variables.

```{r}
all_cols <- colnames(select(slice_wide, -year, -doy, -date))

#variables with log-normal-ish distributions
log_cols <- all_cols[!all_cols %in% c("rh", "temp_mean")]

imp_ts2 <-
  amelia(
    slice_wide,
    m = 10,
    p2s = 0,
    ts = "doy",
    cs = "year",
    intercs = TRUE,
    polytime = 3,
    logs = log_cols,
    idvars = c("date"),
    empri = .01 * nrow(slice_wide), #ridge penalty because of high degree of missingness
    parallel = "multicore",
    ncpus = detectCores() - 1
  )
```
```{r}
imp_ts2
```

## Add lags and leads

```{r}
imp_ts3 <-
  amelia(
    slice_wide,
    m = 10,
    p2s = 0,
    ts = "doy",
    cs = "year",
    intercs = TRUE,
    polytime = 3,
    leads = all_cols,
    lags = all_cols,
    logs = log_cols,
    idvars = c("date"),
    empri = .01 * nrow(slice_wide), #ridge penalty because of high degree of missingness
    parallel = "multicore",
    ncpus = detectCores() - 1
  )
```
```{r}
imp_ts3
```

# Diagnostics

## Compare distributions
These plots just compare distributions of observed and imputed data.  Observed = blue, imputed = red

```{r}
plot(imp_ts1)
plot(imp_ts2, which.vars = c("dimona", "porto_alegre", "km_clust", "colosso_clust"))
plot(imp_ts3, which.vars = c("dimona", "porto_alegre", "km_clust", "colosso_clust"))
```
Huh, here it seems like imputation tends to overestimate precipitation possibly.  It seems odd to me that the greatest mismatch between observed and imputed is in `km_clust` which has the lowest degree of missingness.

## Dispersion test

This diagnostic checks if starting values are likely to effect convergence.  All lines should end up converging at the same place.

```{r}
disperse(imp_ts1, dims = 1, m = 10)
disperse(imp_ts2, dims = 1, m = 10)
disperse(imp_ts3, dims = 1, m = 10)
```

The models with more variables are not converging quickly, but they are all converging on the same value, so that's good.  It's possible that this won't be a problem once the full data set is used because there will be many more observations.

## Overimputation

This test removes an observation that is known, imputes it, then plots the observed vs. imputed.  A perfect result would be points that follow the line perfectly.  Color represents row-wise missingness. (rows with fewer co-variates probably should perform worse).

```{r}
overimpute(imp_ts1, var = "dimona")
overimpute(imp_ts2, var = "dimona")
overimpute(imp_ts3, var = "dimona")
```

They actually seem fairly equivalent.  Imputation tends to underestimate actual precip, but 90% confidence intervals mostly overlap line.  I wonder if this is affected by the ridge penalty?

```{r}
overimpute(imp_ts1, var = "km_clust")
overimpute(imp_ts2, var = "km_clust")
overimpute(imp_ts3, var = "km_clust")
```

Can't tell, but maybe adding lags helped? Adding more variables helps more than adding lags and leads.

## Timeseries Plots

We can also pick a few year/site combos to compare the 4 methods

```{r}
tscsPlot(imp_ts1, cs ="1995", var = "km_clust", ylim = c(0,150))
tscsPlot(imp_ts2, cs ="1995", var = "km_clust", ylim = c(0,150))
tscsPlot(imp_ts3, cs ="1995", var = "km_clust", ylim = c(0,150))
```

```{r}
tscsPlot(imp_ts1, cs ="1997", var = "dimona", ylim = c(0,150))
tscsPlot(imp_ts2, cs ="1997", var = "dimona", ylim = c(0,150))
tscsPlot(imp_ts3, cs ="1997", var = "dimona", ylim = c(0,150))
```

```{r}
tscsPlot(imp_ts1, cs ="1997", var = "colosso_clust", ylim = c(0,150))
tscsPlot(imp_ts2, cs ="1997", var = "colosso_clust", ylim = c(0,150))
tscsPlot(imp_ts3, cs ="1997", var = "colosso_clust", ylim = c(0,150))
```

It doesn't seem like one of these methods is consistently better than others.

# Calculate SPI with imputed dataset

I want to apply the SPI calculation on all the BDFFP sites for all the multiple imputations, then combine them and look at the coefficient of variation or something to see how tight the estimates are.

Here's where I need to figure out how to use this data class.  Need to go back to the Amelia manual.

```{r}
class(imp_ts3)
names(imp_ts3$imputations)
```


```{r}
imp_spi <- 
  map(imp_ts3$imputations, ~{
    .x %>% 
      select(date, dimona, porto_alegre, colosso_clust, km_clust) %>% 
      mutate(yearmonth = tsibble::yearmonth(date)) %>% 
      group_by(yearmonth) %>% 
      summarize(across(-date, ~sum(.x, na.rm = TRUE))) %>% 
      mutate(across(-yearmonth, ~as.numeric(SPEI::spi(.x, scale = 3)$fitted))) %>%
      filter(complete.cases(.))
  }) %>% 
  bind_rows(.id = "imp")

imp_spi_long <-
  imp_spi %>% 
  pivot_longer(dimona:km_clust, names_to = "site", values_to = "spi") %>% 
  mutate(date = as_date(yearmonth))

imp_mean <-
  imp_spi_long %>% 
  group_by(yearmonth, site) %>% 
  summarize(spi_mean = mean(spi)) %>% 
  mutate(date = as_date(yearmonth))
```

- spi <= -2.0 ~ "extreme",
- -2.0 < spi <= -1.5  ~ "severe",
- -1.5 < spi <= -1.0 ~ "moderate",
- -1.0 < spi <= 0 ~ "mild"

```{r}
spi_imp_plot <-
  ggplot(imp_spi_long, aes(x = date, y = spi)) +
  geom_rect(aes(xmin = min(date), xmax = max(date), ymin = -4.2, ymax = -2), fill = "red", alpha = 0.01) +
  geom_rect(aes(xmin = min(date), xmax = max(date), ymin = -2, ymax = -1.5), fill = "darkorange", alpha = 0.01) +
  geom_rect(aes(xmin = min(date), xmax = max(date), ymin = -1.5, ymax = -1), fill = "orange", alpha = 0.01) +
  geom_rect(aes(xmin = min(date), xmax = max(date), ymin = -1, ymax = 0), fill = "yellow", alpha = 0.01) +
  geom_line(aes(group = imp), alpha = 0.1) +
  geom_line(data = imp_mean, aes(x = date, y = spi_mean), color = "blue") +
  facet_wrap(~site) +
  scale_x_date(date_breaks = "2 years", date_labels = "%Y") +
  scale_y_continuous(limits = c(-4.2, 2)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "SPI calculated from multiply-imputed data", y = "SPI", x = "Date", caption = "Grey lines = individual imputations, blue line = mean, red = extreme, dark orange = severe, orange = moderate, yellow = mild")
spi_imp_plot
```

```{r}
ggsave(here("notes", "spi_imputed.png"), spi_imp_plot)
```

# Compare to replacement rules

```{r}
repl <- read_csv(here("data_cleaned", "mon_precip_spi_repl.csv"))

repl1 <-
  repl %>%
  separate(yearmonth, into = c("year", "month"), sep = " ") %>% 
  mutate(date = mdy(paste(month, "1,", year))) %>% 
  filter(between(date, min(imp_spi_long$date), max(imp_spi_long$date)))

repl2 <-
  repl1 %>%
  select(date, site, spi) %>% 
  pivot_wider(names_from = site, values_from = spi) %>% 
  rowwise() %>% 
  mutate(km_clust = mean(c(km37, km41), na.rm = TRUE),
         colosso_clust = mean(c(florestal, gaviao, cabo_frio, colosso))) %>% 
  select(date, colosso_clust, dimona, km_clust, porto_alegre) %>% 
  pivot_longer(-date, names_to = "site", values_to = "spi")
```


```{r}
spi_rules_plot <- 
  ggplot(repl2, aes(x = date, y = spi)) +
  geom_rect(aes(xmin = min(date), xmax = max(date), ymin = -4.2, ymax = -2), fill = "red", alpha = 0.01) +
  geom_rect(aes(xmin = min(date), xmax = max(date), ymin = -2, ymax = -1.5), fill = "darkorange", alpha = 0.01) +
  geom_rect(aes(xmin = min(date), xmax = max(date), ymin = -1.5, ymax = -1), fill = "orange", alpha = 0.01) +
  geom_rect(aes(xmin = min(date), xmax = max(date), ymin = -1, ymax = 0), fill = "yellow", alpha = 0.01) +
  geom_line(color = "blue") +
  facet_wrap(~site) +
  scale_x_date(date_breaks = "2 years", date_labels = "%Y") +
  scale_y_continuous(limits = c(-4.2, 2)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "SPI calculated from replacement-rule data", y = "SPI", x = "Date", caption = "red = extreme, dark orange = severe, orange = moderate, yellow = mild")
ggsave(here("notes", "spi_rules.png"), spi_rules_plot)
```


# Next Steps

## Spatially explicit imputation??

`Ameila` is not spatially explicit.  That is, it does not take distance between sites into account when imputing.  The CRAN Task View on missing data does list some packages for spatio-temporal imputation, but they look less developed than Ameila.


## Calculate SPI with multiple imputations

See how much uncertainty there is for SPI calculations with multiple imputation.  That is, do multiple imputations, calculate SPI, and re-combine.

## Scale up

So far, I've only run the `Amelia` diagnostics on imputation from a small slice of the precip data.  I should run it on the full dataset and see how it performs.
