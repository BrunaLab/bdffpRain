---
title: "Wrangling Precipitation Data"
author: "Eric R. Scott"
date: "2020-06-24"
output: 
  html_notebook: 
    highlight: kate
    theme: yeti
    toc: yes
    toc_float: yes
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
library(conflicted)
library(readxl)
library(lubridate)
library(janitor)
library(hms)
library(tsibble)
conflict_prefer("filter", "dplyr")
conflict_prefer("lag", "dplyr")
```

*Last compiled: `r Sys.Date()`*

# Purpose

Reading and wrangling precip data

# Reading and parsing data

The HORA column is sometimes formatted as text and sometimes as time.  Read in both ways and join.

```{r data, echo=TRUE}
sites <- list.files(here("data_raw", "bdffp"))
names(sites) <- str_remove(sites, "\\..+$")

all_raw <- map_df(sites, ~{
  site <- .x
  sheets <- excel_sheets(here("data_raw", "bdffp", site))
  sheets <- sheets[str_detect(sheets, "\\d{4}")]

  site_raw <-
    map_df(sheets, ~ read_excel(
      here("data_raw", "bdffp", site),
      sheet = .x,
      range = cell_limits(c(3,1), c(NA, 6)),
      col_types = c("date", "numeric", "text", "numeric", "text", "text")
    ))

time_as_date <-
  map_df(sheets, ~ suppressWarnings(read_excel(
    here("data_raw", "bdffp", site),
    sheet = .x,
    range = cell_limits(c(3,1), c(NA, 6)),
    col_types = c("date", "numeric", "text", "numeric", "date", "text")
  )))

site_raw <- site_raw %>% add_column(time_as_date = time_as_date$HORA)
},
.id = "site")
sample_n(all_raw, 7)
```

Add row numbers to raw data to make data cleaning more reproducible later

```{r}
all_raw <- all_raw %>% rowid_to_column()
```


## Data Dictionary

There are 8 sites, each with a .xls spreadsheet with multiple tabs.  Each tab contains data from 3 years and the last two tabs have some aggregated data done in Excel.

- `DATA`: Date
- `CHUVA`: Rainfall in mm (?)
- `OBSERVADOR`: the name of the person who recorded the data
- `DIA`: day of year
- `HORA`: time of collection in 24-hr time  (sometimes as text, sometimes as time)
- `COMENTARIO`: notes.  For example, whether the data was aggregated over previous days because no one was around to check the gauge.



# Cleaning

## Basics

- Remove missing rows
- Make column headings lowercase
- combine columns that are just typos (comentario and commentario)

```{r}
all <-
  all_raw %>% 
  clean_names() %>% 
  mutate(comentario = ifelse(is.na(comentario), commentario, comentario),
         data = as_date(data)) %>% 
  select(-commentario)
all

#fix typo in date column
all %>% filter(is.na(data) & !is.na(chuva))
all <- all %>% mutate(data = replace(data, rowid == 6646, ymd("2000-03-31")))

#remove missing rows
all <- all %>% filter(!is.na(data))
```

## Site names
- Change to lowercase
- floresta -> florestaL (missing L at end)
- 41dia -> km41
- Cabo -> Cabo Frio
- Porto -> Porto Alegre
- 37 -> km37

```{r}
all <-
  all %>% 
  mutate(site = str_to_lower(site)) %>% 
  mutate(site = fct_recode(site,
                           florestal = "floresta",
                           km41 = "41dia",
                           "cabo frio" = "cabo",
                           "porto alegre" = "porto",
                           km37 = "37")) %>% 
  #change case of observer to be consistent
  mutate(observador = str_to_title(observador))
```


## Quality Control

### Explicit NAs entered as 0

Some observations have 0 entered for precip, but "nao foi feita observ" (no observation made) entered in notes.

```{r}
# all %>% filter(comentario == "nao foi feita observ")
all <-
  all %>% 
  mutate(chuva = replace(chuva, str_detect(comentario, "nao foi feita observ"), NA))
```

### Check that there are no missing dates

This shows any problems with dates by checking the data against a sequence from earliest to latest date by day.

- the missing date in COLOSSO is a typo that will get fixed later

```{r}
x <- all %>% 
  group_by(site) %>% 
  summarize(start = min(data),
            end = max(data))
out <- list()
for (i in 1:nrow(x)) {
  out[[i]] <- tibble(date = seq(x$start[i], x$end[i], by = "days"))
}
names(out) <- x$site
tester <- bind_rows(out, .id = "site")

missing_dates <- anti_join(tester, all, by = c("site" = "site", "date" = "data")) 

missing_dates %>% 
  group_by(site) %>% 
  summarize(number_missing = n(),
            min = min(date),
            max = max(date))

# missing_dates %>% filter(site %in% c("41dia", "colosso"))
```

- Check that `DIA` matches day of year calculated from date.

```{r}
all %>% 
  mutate(doy = yday(data)) %>% 
  #show only rows where the calculated DOY doesn't match the entered DOY
  filter(dia != doy) %>% 
  group_by(site) %>% 
  summarize(n = n(),
            start = min(data),
            end = max(data))
```

A few are (probably) typos,  CABO, DIMONA, and GAVIAO are off by one day for large sections, and I don't know what is going on in PORTO.  This column should be trashed and replaced by `yday(date)`.

```{r}
all <-
  all %>% 
  mutate(dia = yday(data))
```

### Leading and Trailing NAs

Remove any trailing NAs in the dataset.

```{r}
all <- 
  all %>% 
  group_by(site) %>% 
  mutate(cum_not_na = cumsum(!is.na(chuva))) %>% 
  mutate(trail = max(cum_not_na) - cum_not_na <=0 & is.na(chuva)) %>% 
  filter(!trail) %>% 
  #leading NAs:
  filter(cum_not_na > 0)
```


### Check times

For the HORA column in Excel, sometimes the entry is character and sometimes it is a time.  When `read_excel` uses "text", then the character entries read in correctly, but the time entries read in as a weird decimal number.  When `read_excel()` uses "date", then the character entries read in as `<NA>` and the time entries read in as the correct time, but on the date 1899-12-31. There are also some text entries of time that include notes, for example, "7:00    chuva noite<0,1", or are entirely notes, e.g. "nao foi feita observ". 

1. Extract notes and times separately from the `hora` column.
2. Convert the `hora` column to time (as HH:MM)
3. Extract any notes written in the time column and merge with other notes.


```{r}
all_2 <-
  all %>% 
  mutate(time_notes = str_remove(hora, "\\d{1,2}:\\d{2}"), #everything except times
         hora = str_replace(hora, "(?<=\\d{1,2});(?=\\d{2})", ":"), #fix semi-colon typos
         time_from_text = as_datetime(parse_hm(str_extract(hora, "\\d{1,2}:\\d{2}")))) %>% #convert to time
  mutate(time_notes = ifelse(!is.na(time_as_date), NA, time_notes), #get rid of notes that shouldn't be there
         time_notes = na_if(time_notes, "")) %>% 
  #merge two dates and format as HH:MM
  mutate(time = as_datetime(ifelse(!is.na(time_as_date), time_as_date, time_from_text)),
         time = format(time, "%R"))
```

Check that you got them all by comparing original data read in both ways to new time column and look for NAs.

```{r}
all_2 %>% 
  filter(!is.na(hora) & is.na(time))
```
There are maybe a couple of others I could guess at, but I don't think time is going to be an important variable, so this is good enough.

### Consolidate comments

Combine notes entered into the wrong columns all into the notes column.

```{r}
all_clean <-
  all_2 %>% 
  unite(notes, c(time_notes, comentario), na.rm = TRUE, sep = "; ") %>% 
  mutate(notes = na_if(notes, ""))
all_clean
```

### Translate

Translate column names to English

```{r}
all_clean2 <-
  all_clean %>% 
  select(rowid, site, date = data, doy = dia, time, observer = observador, precip = chuva, notes) %>% 
  ungroup()
```
```{r}
skimr::skim(all_clean2)
unique(all_clean2$notes)
```


# Check for problems

## Problems in precip

The value for DIMONA 2005-02-22 is not correct.  It got converted to a date by Excel.  Dr. Bruna did some investigating and the correct value is 53

```{r}
all_clean2 %>% filter(precip < 0)
all_clean2 %>% filter(precip > 500)
#typo in Dimona
all_clean2 <- 
  all_clean2 %>% 
  mutate(precip = replace(precip, rowid == 28040, 53))
```
## Problems with dates

Check if dates are in order and if any are missing:

```{r}
#Dates not in order
all_clean2 %>% 
  group_by(site) %>% 
  filter(date != lag(date) + 1 & !is.na(lag(date)))
```
Duplicated and empty dates

```{r}
all_clean2 %>% 
  filter(between(rowid, 10700, 10710))
#duplicated dec 1

all_clean2 <- all_clean2 %>% filter(rowid != 10703)
```

### Legitimate gaps in data
Cabo Frio

```{r}
#11689
all_clean2 %>% 
  filter(between(rowid, 11687, 11692))
```

Florestal

```{r}
#30573
all_clean2 %>% 
  filter(between(rowid, 30570, 30578))
# Years entered backwards, but ok

#30669	
all_clean2 %>% 
  filter(between(rowid, 30665, 30673))

#31065
all_clean2 %>% 
  filter(between(rowid, 31062, 31070))
```

Gaviao

```{r}
#37498
all_clean2 %>% 
  filter(between(rowid, 37495, 37501))
```

Porto Alegre
```{r}
#39003
all_clean2 %>% 
  filter(between(rowid, 39000, 39006))

```



### Typos That need fixing

Typos in date

Colosso
```{r}
#12826
all_clean2 %>% 
  filter(between(rowid, 12824, 12830))
```

Gaviao
```{r}
#32907
all_clean2 %>% 
  filter(between(rowid, 32904, 32910))
```

Porto Alegre
```{r}
#39430
all_clean2 %>% 
  filter(between(rowid, 39427, 39433))
# should be 1992-10-20

#39889
all_clean2 %>% 
  filter(between(rowid, 39886, 39892))
# typo. 39889 should be 1994-01-05
# 39891 should be 1994-01-07
```

```{r}
typos <- c("12826" = ymd("1988-10-01"),
           "32907" = ymd("1992-10-14"),
           "39430" = ymd("1992-10-20"),
           "39889" = ymd("1994-01-05"),
           "39891" = ymd("1994-01-07"))

all_clean2 <- 
  all_clean2 %>% 
  mutate(date = replace(date, rowid %in% as.numeric(names(typos)), typos))
```


## Duplicated dates

There shouldn't be any duplicates at this point.

```{r}
duplicates(all_clean2, key = site, index = date)
```

# Flags

I think it would be useful to create a `flag` column using the information in the notes.  I should research if there are some standards for how to do this already (like, what letters to use).  There are also some notes that relate to data directly possibly.

Flags:

- `A`: tagged accumulation
- `U`: possible untagged accumulations
- `E`: data error
- `T`: trace precipitation

## Flags from notes
Pull info from notes to flag data

```{r}
notes <- all_clean2$notes %>% unique()
notes
```

## Untagged accumulations
Using the data, find possible untagged accumulations as data points that:

- Are larger than 0
- Follow a gap (`NA`s)
- Are not the first record at a site

```{r}
all_clean2 %>% 
  group_by(site) %>% 
  filter(rowid == first(rowid))
all_clean2 %>% 
  group_by(site) %>% 
  filter(precip > 0 & is.na(lag(precip)) & is.na(flag) & rowid != first(rowid))
```

## Create flags

```{r}
all_flags <- 
  all_clean2 %>% 
  group_by(site) %>% 
  mutate(flag = case_when(
    str_detect(notes, "(A|a|Á)c{1,2}umu") ~ "A",
    str_detect(notes, "(P|p)luviometro no ch") ~ "E",
    str_detect(notes, "errada") ~ "E",
    str_detect(notes, "chuva noite<0,1") ~ "T",
    precip > 0 & is.na(lag(precip)) & rowid != first(rowid) ~ "U",
    TRUE ~ NA_character_
  ))

#check that "U" didn't overwrite "A"
# all_flags %>% filter(flag =="U")
# all_flags %>% filter(flag == "A")
```


# Write to file

```{r}
#remove rowname
out <- all_flags %>% select(-rowid)
write_csv(out, here("data_cleaned", "daily_precip.csv"))
```
