---
title: "Gridded data wrangling"
author: "Eric R. Scott"
date: "2020-08-12"
output: 
  html_notebook: 
    highlight: kate
    theme: yeti
    toc: yes
    toc_float: yes
    number_sections: yes
---

**TODO:**

- Explore usefulness of statistical downscaling of gridded data using the [rainfarmr](https://github.com/jhardenberg/rainfarmr) package

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
library(conflicted)
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
library(ncdf4)
library(raster)
library(lubridate)
library(sp)
```

*Last compiled: `r Sys.Date()`*


# South America gridded data

This one is not available through OPENDAP as far as I can tell.  

Liebmann, Brant, and Dave Allured. “Daily Precipitation Grids for South America.” Bulletin of the American Meteorological Society 86, no. 11 (November 2005): 1567–70. https://doi.org/10.1175/BAMS-86-11-1567.

https://psl.noaa.gov/data/gridded/data.south_america_precip.html

## Download

```{r}
if (!file.exists(here("data_raw", "sa24.daily.1.nc"))) {
  download.file("ftp://ftp.cdc.noaa.gov/Datasets.other/south_america/sa24.daily.1.1940-2012.nc",
                here("data_raw", "sa24.daily.1.nc"))
}
```
```{r}
sa <- nc_open(here("data_raw", "sa24.daily.1.nc"))
```
```{r}
names(sa$var)
names(sa$dim)

lat <- ncvar_get(sa, "lat")
lon <- ncvar_get(sa, "lon")
time <- ncvar_get(sa, "time") #"days since 1800-1-1 00:00:00"

time <- as_date(time, origin = ymd("1800-01-01"))
```

## Filter location and time

I want the grid cell centered on -2 lat and -60 lon.  That will encompass all of BDFFP, I think.

```{r}
lat_i = which(lat == -2)
lon_i = which(lon == -60)
tstart_i <- which(time == ymd("1987-09-01"))
tend_i <- length(time)

precip <- ncvar_get(sa, "precip",
                    start = c(lon_i, lat_i, tstart_i), # c(lon, lat, time)
                    count = c(1, 1, tend_i - tstart_i + 1)  # c(lon, lat, time)
                    )
```

## Tidy

```{r}
dim(precip)
length(tstart_i:tend_i)

out <- tibble(date = time[tstart_i:tend_i], lat = -2, lon = -60, precip = precip)
out
write_csv(out, here("data_cleaned", "sa_daily_1x1.csv"))
```

# Xavier et al.

Gridded data that includes evapotranspiration, relative humidity, temperature, and precip.  Resolution is 0.25ºx0.25º.

Found here: http://careyking.com/data-downloads/

Xavier, Alexandre C., Carey W. King, and Bridget R. Scanlon. “Daily Gridded Meteorological Variables in Brazil (1980–2013).” International Journal of Climatology 36, no. 6 (2016): 2644–59. https://doi.org/10.1002/joc.4518.

The data comes in decade long files with one variable per file (UGH).  I'm using v2.1, not the most recent, but more complete than v2.2


## Merge NetCDF files

Merging the NetCDF files is going to be most easily done with a command line tool, `cdo`.  Info on installation here: https://code.mpimet.mpg.de/projects/cdo

### Precip

This code filters each file for only the relevant grid cells, then merges them and writes a file `all_precip.nc`

```{bash eval=FALSE}
cd ~/Documents/Heliconia-Drought/data_raw/XavierUT

cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 prec_daily_UT_Brazil_v2.1_19800101_19891231.nc 1980_1989.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 prec_daily_UT_Brazil_v2.1_19900101_19991231.nc 1990_1999.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 prec_daily_UT_Brazil_v2.1_20000101_20091231.nc 2000_2009.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 prec_daily_UT_Brazil_v2.1_20100101_20151231.nc 2010_2015.nc

cdo -O mergetime 1980_1989.nc 1990_1999.nc 2000_2009.nc 2010_2015.nc all_precip.nc
rm 1980_1989.nc 1990_1999.nc 2000_2009.nc 2010_2015.nc
```

### ETo

Repeat with evapotranspirtation

```{bash eval=FALSE}
cd ~/Documents/Heliconia-Drought/data_raw/XavierUT

cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 ETo_daily_UT_Brazil_v2_19800101_19891231.nc 1900_1989.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 ETo_daily_UT_Brazil_v2_19900101_19991231.nc 1990_1999.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 ETo_daily_UT_Brazil_v2_20000101_20061231.nc 2000_2006.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 ETo_daily_UT_Brazil_v2_20070101_20131231.nc 2007_2013.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 ETo_daily_UT_Brazil_v2_20140101_20170731.nc 2014_2017.nc

cdo -O mergetime 1900_1989.nc 1990_1999.nc 2000_2006.nc 2007_2013.nc 2014_2017.nc all_eto.nc
rm 1900_1989.nc 1990_1999.nc 2000_2006.nc 2007_2013.nc 2014_2017.nc
```

```{bash eval=FALSE}
cd ~/Documents/Heliconia-Drought/data_raw/XavierUT

cdo -O merge all_precip.nc all_eto.nc precip_eto.nc
```


## Tidy

```{r}
xa_all <- nc_open(here("data_raw", "XavierUT", "precip_eto.nc"))

time <- ncvar_get(xa_all, "time") #hours since 1980-01-01 12:00:00
time <- as_date(time/24, origin = ymd("1980-01-01"))

prec <- ncvar_get(xa_all, "prec")
eto <- ncvar_get(xa_all, "ETo")
dim(prec)

dimnames(eto) <- dimnames(prec) <- list(ncvar_get(xa_all, "longitude"),
                       as.character(time))

xa_precip <-
  t(prec) %>% #gotta do this because dates aren't unique and can't be colnames
  as_tibble(rownames = "date") %>% 
  pivot_longer(-date, names_to = "lon", values_to = "precip") 

xa_eto <-
  t(eto) %>% #gotta do this because dates aren't unique and can't be colnames
  as_tibble(rownames = "date") %>% 
  pivot_longer(-date, names_to = "lon", values_to = "eto")

xa_clim <-
  full_join(xa_precip, xa_eto, by = c("date", "lon")) %>% 
  add_column(lat = ncvar_get(xa_all, "latitude")) %>% 
  mutate(date = ymd(date), lon = as.numeric(lon)) %>% 
  select(date, lat, lon, precip, eto)
```

```{r}
write_csv(xa_clim, here("data_cleaned", "xavier_daily_0.25x0.25.csv"))
```

# TRMM

Wow, really having a lot of trouble with this.  Their web-interface lets me subset by time and location, but just gives me a list of links and instructions to use `curl` to download them all.  Then I'd have to stitch them together.

## Link list:

```{r}
tail(read_lines(here("data_raw", "subset_TRMM_3B43_7_20201120_205339.txt")))
```



## OPeNDAP

They have a THREDDS server, but `nc_open` either can't deal with HDF files or something else is wrong.

```{r}
nc_open("https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3B43.7/1998/3B43.19980101.7.HDF")
```

## rgdal

`rgdal` is an interface to the command-line tool `gdal`. This also seems to not work.

```{r}
library(rgdal)
library(gdalUtils)

sds <- get_subdatasets("https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3B43.7/1998/3B43.19980101.7.HDF.nc4?precipitation[479:481][190:190],relativeError[479:481][190:191],gaugeRelativeWeighting[479:481][190:191],nlon[479:481],nlat[191:191]")
```
## NASAaccess

NASA provides an R package, but it requires a shapefile (.shp) to subset data by location.  Not sure if I could give it a `raster::extent()` or `sp::bbox()`, but seems like probably not.

```{r}
library(NASAaccess)
?NASAaccess::GPMpolyCentroid
```


# CHIRPS

Also doesn't work.  The README from `climateR` suggests that this should work, but it doesn't.  Seems like a bug https://github.com/mikejohnson51/climateR/issues/22

```{r}
library(climateR)
library(sf)
AOI <- st_bbox(c(xmin = -60, xmax = -59, ymin = -2.5, ymax = -2), crs = 4326)

getCHIRPS(AOI, startDate = "1999-01-01", endDate = "2000-01-01")

```



