---
title: "Gridded data wrangling"
author: "Eric R. Scott"
date: "2020-08-12"
output: 
  html_notebook: 
    highlight: kate
    theme: yeti
    toc: yes
    toc_float: yes
    number_sections: yes
---

**TODO:**

- Explore usefulness of statistical downscaling of gridded data using the [rainfarmr](https://github.com/jhardenberg/rainfarmr) package

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
library(conflicted)
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
library(ncdf4)
library(raster)
library(lubridate)
library(sp)
library(glue)
```

*Last compiled: `r Sys.Date()`*


# South America gridded data

This one is not available through OPENDAP as far as I can tell.  

Liebmann, Brant, and Dave Allured. “Daily Precipitation Grids for South America.” Bulletin of the American Meteorological Society 86, no. 11 (November 2005): 1567–70. https://doi.org/10.1175/BAMS-86-11-1567.

https://psl.noaa.gov/data/gridded/data.south_america_precip.html

## Download

```{r}
if (!file.exists(here("data_raw", "sa24.daily.1.nc"))) {
  download.file("ftp://ftp.cdc.noaa.gov/Datasets.other/south_america/sa24.daily.1.1940-2012.nc",
                here("data_raw", "sa24.daily.1.nc"))
}
```
```{r}
sa <- nc_open(here("data_raw", "sa24.daily.1.nc"))
```
```{r}
names(sa$var)
names(sa$dim)
sa
lat <- ncvar_get(sa, "lat")
lon <- ncvar_get(sa, "lon")
time <- ncvar_get(sa, "time") #"days since 1800-1-1 00:00:00"

time <- as_date(time, origin = ymd("1800-01-01"))
```

## Filter location and time

I want the grid cell centered on -2 lat and -60 lon.  That will encompass all of BDFFP, I think.

```{r}
lat_i = which(lat == -2)
lon_i = which(lon == -60)
tstart_i <- which(time == ymd("1987-09-01"))
tend_i <- length(time)

precip <- ncvar_get(sa, "precip",
                    start = c(lon_i, lat_i, tstart_i), # c(lon, lat, time)
                    count = c(1, 1, tend_i - tstart_i + 1)  # c(lon, lat, time)
                    )
```

## Tidy

```{r}
dim(precip)
length(tstart_i:tend_i)

out <- tibble(date = time[tstart_i:tend_i], lat = -2, lon = -60, precip = precip)
out
write_csv(out, here("data_cleaned", "sa_daily_1x1.csv"))
```

# Xavier et al.

Gridded data that includes evapotranspiration, relative humidity, temperature, and precip.  Resolution is 0.25ºx0.25º.

Found here: http://careyking.com/data-downloads/

Xavier, Alexandre C., Carey W. King, and Bridget R. Scanlon. “Daily Gridded Meteorological Variables in Brazil (1980–2013).” International Journal of Climatology 36, no. 6 (2016): 2644–59. https://doi.org/10.1002/joc.4518.

The data comes in decade long files with one variable per file (UGH).  I'm using v2.1, not the most recent, but more complete than v2.2


## Merge NetCDF files

Merging the NetCDF files is going to be most easily done with a command line tool, `cdo`.  Info on installation here: https://code.mpimet.mpg.de/projects/cdo

### Precip

This code filters each file for only the relevant grid cells, then merges them and writes a file `all_precip.nc`

```{bash eval=FALSE}
cd ~/Documents/Heliconia-Drought/data_raw/XavierUT

cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 prec_daily_UT_Brazil_v2.1_19800101_19891231.nc 1980_1989.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 prec_daily_UT_Brazil_v2.1_19900101_19991231.nc 1990_1999.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 prec_daily_UT_Brazil_v2.1_20000101_20091231.nc 2000_2009.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 prec_daily_UT_Brazil_v2.1_20100101_20151231.nc 2010_2015.nc

cdo -O mergetime 1980_1989.nc 1990_1999.nc 2000_2009.nc 2010_2015.nc all_precip.nc
rm 1980_1989.nc 1990_1999.nc 2000_2009.nc 2010_2015.nc
```

### ETo

Repeat with evapotranspirtation

```{bash eval=FALSE}
cd ~/Documents/Heliconia-Drought/data_raw/XavierUT

cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 ETo_daily_UT_Brazil_v2_19800101_19891231.nc 1900_1989.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 ETo_daily_UT_Brazil_v2_19900101_19991231.nc 1990_1999.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 ETo_daily_UT_Brazil_v2_20000101_20061231.nc 2000_2006.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 ETo_daily_UT_Brazil_v2_20070101_20131231.nc 2007_2013.nc
cdo sellonlatbox,-60.167,-59.583,-2.5,-2.25 ETo_daily_UT_Brazil_v2_20140101_20170731.nc 2014_2017.nc

cdo -O mergetime 1900_1989.nc 1990_1999.nc 2000_2006.nc 2007_2013.nc 2014_2017.nc all_eto.nc
rm 1900_1989.nc 1990_1999.nc 2000_2006.nc 2007_2013.nc 2014_2017.nc
```

```{bash eval=FALSE}
cd ~/Documents/Heliconia-Drought/data_raw/XavierUT

cdo -O merge all_precip.nc all_eto.nc precip_eto.nc
```


## Tidy

```{r}
xa_all <- nc_open(here("data_raw", "XavierUT", "precip_eto.nc"))

time <- ncvar_get(xa_all, "time") #hours since 1980-01-01 12:00:00
time <- as_date(time/24, origin = ymd("1980-01-01"))

prec <- ncvar_get(xa_all, "prec")
eto <- ncvar_get(xa_all, "ETo")
dim(prec)

dimnames(eto) <- dimnames(prec) <- list(ncvar_get(xa_all, "longitude"),
                       as.character(time))

xa_precip <-
  t(prec) %>% #gotta do this because dates aren't unique and can't be colnames
  as_tibble(rownames = "date") %>% 
  pivot_longer(-date, names_to = "lon", values_to = "precip") 

xa_eto <-
  t(eto) %>% #gotta do this because dates aren't unique and can't be colnames
  as_tibble(rownames = "date") %>% 
  pivot_longer(-date, names_to = "lon", values_to = "eto")

xa_clim <-
  full_join(xa_precip, xa_eto, by = c("date", "lon")) %>% 
  add_column(lat = ncvar_get(xa_all, "latitude")) %>% 
  mutate(date = ymd(date), lon = as.numeric(lon)) %>% 
  select(date, lat, lon, precip, eto)
```

```{r}
write_csv(xa_clim, here("data_cleaned", "xavier_daily_0.25x0.25.csv"))
```

# TRMM

My goodness this was difficult to do.  I ended up following their instructions for using GET requests and combined data with non-spatial wrangling tools I know how to use already.

## Configure access to opendap

Instructions [here](https://wiki.earthdata.nasa.gov/display/EL/How+to+access+data+with+R) for setting up access via http GET requests.  It involves creating a .netrc file to hold config information

```{r}
library(httr)
set_config(
  config(
    followlocation = 1,
    netrc = 1,
    netrc_file = here(".netrc"),
    cookie = here(".urs_cookies"),
    cookiefile = here(".urs_cookies"),
    cookiejar = here(".urs_cookies")
  )
)
```

## Download files

Get list of download links [here](https://disc.gsfc.nasa.gov/datasets/TRMM_3B43_7/summary) using the "Subset / Get Data" link.

```{r}
urls <- read_lines(here("data_raw", "TRMM", "subset_TRMM_3B43_7_20201130_204909.txt"), skip = 1)
urls[1]
```

Extract file name and create paths to write to.

```{r}
dates <- str_extract(urls, "(?<=3B43\\.)\\d{8}")
outnames <- glue('trmm_3B43_7_{dates}.nc4')
outpaths <- here("data_raw", "TRMM", outnames)
```

Download each .nc4 file.

```{r}
map2(urls, outpaths, ~{
  GET(url = .x,
      write_disk(.y))
})
```

## Read in and combine

I did this before with a command line utility, but someone suggested `raster`.  In fact, these .nc4 files don't have a time dimension, so combining them with the command line utility doesn't seem to work.  I'm going to take a fairly hacky solution with `purr::map`, which is fine since I'm not using these data in a spatial analysis and want them in a tidy format anyway.

```{r}
r <- raster(outpaths[1], crs = 4326)
plot(r)
```
For some weird reason, the data are rotated before I do any processing.  I'm not the only one to have [this problem](https://stackoverflow.com/questions/44507754/raster-stack-incorrecting-plotting-latitude-and-longitude-coordinates) with TRMM data.

```{r}
r.t <- t(r) #rotate
r.t.flipy <- flip(r.t, direction = 2)
r.t.flipxy <- flip(r.t.flipy, direction = 1)

plot(r.t.flipxy)
```

Read in all files, flip them, extract precip data as a matrix.

```{r}
precip_list <-
  map(outpaths, ~{
    r <- raster(.x) %>% 
      t() %>% flip(2) %>% flip(1) 
    
    p <- getValues(r, format = "matrix")
    colnames(p) <- xFromCol(r)
    rownames(p) <- yFromRow(r)
    p
  }) %>% set_names(ymd(dates))
```

## Tidy

Take that list of matrixes and convert to tidy data frame

```{r}
trmm <-
  precip_list %>% 
  map_dfr(~{
    as_tibble(.x, rownames = "lat") %>% 
      pivot_longer(-lat, names_to = "long", values_to = "precip")
  }, .id = "date") %>% 
  mutate(date = ymd(date))
```

test plot:
```{r}
trmm %>% 
  filter(lat == first(lat), long == first(long)) %>% 
  ggplot(aes(x = date, y = precip)) + geom_line()
```


## Write to file

```{r}
write_rds(trmm, here("data_cleaned", "trmm.rds"))
```



